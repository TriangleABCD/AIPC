# AIPC 现状调研

## 1. 引言

近年来，**AIPC** 的概念愈发流行。越来越多的个人 PC 设备被冠以 AIPC 的名号，各大硬件供应商也越来越将 AI 支持作为其硬件特性大力宣传，以及各种基础设施和软件社区也经常提到 AIPC 话题。这是人工智能不断发展、逐渐融入传统计算机各方面的大趋势。

想要给 AIPC 下一个精确的定义，是十分困难且无意义的。从字面意义来理解，AIPC 即配备有 AI 功能的 PC。然而，“AI 功能”如何界定？面向用户的 AI 服务当然算，但是面向开发者的 AI 开发功能也可以被包括。另外，一项 AI 服务实际可能同时分布在终端设备和云端服务，是否都属于 AIPC 的范畴呢？本文不期望得到一个严谨的定义，而是从不同的角度去阐述 AIPC。

## 2. AI on PC

AIPC 最直接的含义就是 AI on PC，表示配备有一定 AI 功能的个人计算设备，这是最直观体现 AIPC 的角度。

自面向个人计算机的操作系统诞生初期，就有了收集用户使用数据来提供更加个性化功能服务的程序，例如 office97 的“大眼夹”助手，又例如各种视频、音乐软件的个性化推送功能。此时 AI 功能的主要特点是收集和分析数据。

在深度学习流行后，面向用户的 AI 服务在软件层面得到了极大的加强，体现在更加丰富的 AI 功能、更高的软件集成度以及更加智能个性化的推荐算法。得益于计算硬件的发展，越来越的的软件功能开始基于 AI 实现，例如各种图像视频算法、语音识别、AI 翻译等等。此时 AI 服务的主要特点是着眼于提升各种软件功能，但是各类算法已经相对完善。

近两年来，大模型的应用开始深入各个领域，AI 服务也逐渐向硬件层面拓展，正逐渐完善用户的 AI 体验，真正向 AIPC 靠拢。

一方面，AI 服务已经不局限于收集信息并提供功能服务的层面，而是有主动判断需求、执行任务的功能。例如集成在 Windows 系统的 Copilot，可以通过文本输入获取复杂的用户需求之后，直接对操作系统和一些应用软件做出操作，无需用户自己再手动进行操作。

另一方面，相关硬件设备使得用户和 AI 交互的效率大大提升。例如用于各种图像算法的高清摄像头、用于各种 AI 语音功能的麦克风设备、更进阶的用于识别深度信息的激光雷达。如今笔记本计算机键盘上配备的 Copilot 按键也是一个体现。AI on PC 的软硬件结合更加紧密，用户从硬件接口直达软件 AI 服务。

另一方面，对于 AI 开发者而言，AIPC 应当具备基本的 AI 开发功能，主要指的是 AI 开发过程中的模型训练、调优、测试、部署等步骤，都可以在 AIPC 上完成。

## 3. AIPC 和基础设施

传统的云计算平台的整个架构是围绕虚拟化的，后来也延伸了裸金属和容器的管理，但整体的思路依旧是以虚拟化为中心。然而，AI 业务场景使得传统的范式越来越难以应对新的需求。

### 3.1 AI 场景下云计算平台的变化

AI 业务场景下的云基础设施的变化主要可以从计算、存储和通信这三个方面来概括。

从计算范式上看， AI 业务最明显的特征就是计算中心从 CPU 转向了 GPU。Intel 2005 年发布的 VT-x 是 CPU 划时代的进展，它代表着虚拟化由纯软模拟转向了硬件辅助。随着 Intel、AMD 的数据中心 CPU 对虚拟化有了越来越好的支持 (VT-x、VT-d、EPT、VMCS、ACPIv、AVIC)，虚拟化的 CPU 效率和物理 CPU 相比已经不遑多让。

但是 GPU 的虚拟化则没有这么完善。目前有多种 GPU 虚拟化的方案：VGPU、GPU Timeslicing、MIG GPU、MPS、CUDA 劫持等。当前存在这么多的 GPU 虚拟化方案本质上是因为 GPU 的物理特性和工作方法所决定的。

首先看物理特性。GPU 显存的带宽远远大于内存的带宽，因此很多需要借助 CPU 或内存的 GPU 虚拟化方案，在内存带宽和显存带宽这一块的差异会很大。

再就是工作原理，由于 GPU 的核心工作是计算，而大模型的大量参数需要确保首先装载到模型，再上 GPU Context 的切换成本，导致大尺寸模型下“GPU 虚拟化”的价值很微弱，对于小尺寸模型或者低算力的场景下，还可以发挥一点作用。

从存储特点上来看，传统云计算主要的存储对象是块、文件、对象，而 AI 业务的存储对象主要是文件和对象。同时数据的来源、清洗、管理成为非常受关注的部分。块的重要程度有所降低。因为模型文件的巨大，存储缓存更加受关注。

从通信的角度来看，传统业务主要围绕虚拟网卡，而 AI 业务则因为分布式训练需要非常大的通信带宽和通信延迟，而需要考虑更多的通信设备。其中机器内通信主要靠共享内存、PCIe 主线和 NVLink；机器间的通信则依赖于网络传输和 RDMA 技术。

### 3.2 AIPC 和 openEuler RISC-V

基础设施中，操作系统发行版的维护面临这巨大的挑战，主要原因有当前的软件包规模大、涉及领域多，进行自主维护，对人力、能力都有很高的要求。

AI 技术的应用使得软件包的上游同步、打补丁、兼容性检查以及依赖关系的处理变得更加自动化，大大降低了这些步骤的人力需求。

同时，AI 模型可以做到一定程度的服务器自动运维，包括各种自动的智能调优，bug 感知等等。

以 openEuler RISC-V 发行版为例，在原本 openEuler 发行版维护软件包的巨大工作量的基础上，还要增添 RISC-V 架构带来的各种兼容性问题、软件包的修复等等，需要及时移植、修复一些重要的软件包。引入 AI 技术，辅助软件包的引入、构建、兼容性检查、故障反馈，可以大幅降低人工的工作量。

以软件包的修复为例，对于在 RISC-V 架构下构建失败的软件包，NLP 技术可以从冗长的构建信息和报错信息中快速提取关键的文字，以作为人工判断失败原因的依据，甚至直接作为大模型判断一些共性的错误原因的依据。

又例如软件包的移植，很多移植代码的编写是带有重复性和机械性的，与硬件相关的部分通常还带有可读性较差的架构相关参数。大模型可以替代人工编写，阅读相关文档，类比原架构代码，自动生成相应的移植后代码，大大提高了移植的效率。

## 4. AIPC 和 RISC-V

处理器芯片设计是一项很复杂的任务，大众看到的处理器芯片架构，呈现为一组微架构参数，例如核心数、多级缓存的大小、主频等等。但为何是选择这样的配置，不同配置对处理器的的性能、功耗等参数有什么影响？要搞清楚这些联系，则需要一整套处理器架构设计基础设施的支撑，例如程序特征分析技术、设计空间探索技术、高精度模拟器、系统仿真技术、验证技术等等。

AI 技术的快速发展，深刻影响了芯片领域。AI 模型被嵌入芯片设计中以替代传统模块。同时芯片设计过程也得到了 AI 技术的辅助而使得设计更复杂的微架构成为可能。另外，芯片领域的进步也成为了 AI 快速发展的硬件基础。

### 4.1 AI 嵌入芯片

处理器微架构设计优化的一个思路是发现程序行为中的共性特征并进行加速，例如程序的局部性原理，比如缓存机制和预先取指机制充分利用了程序的空间和时间局部性特征。这些优化都是基于程序的结构化设计这一事实，即程序大部分时候顺序执行，并在局部带有循环和分支特征。

然而，针对简单的顺序、分支、循环这些特征的优化模式早就已经被提炼总结，如今要进一步提取更多共性特征已经十分困难，因为有些特征不再是人类能够直观理解，很多表现为统计意义上的特征。AI 技术正是挖掘统计意义特征的有效手段。过去十几年，很多体系结构研究开始考虑在芯片微架构中引入一些借鉴 AI 思想可挖掘统计特征的模块。

以分支预测单元（Branch Prediction Unit, BPU）为例，这是现代高性能处理器的一个重要组成部分，负责根据分支指令执行历史预测分支的走向，从而提前执行对应方向上的指令。BPU 的预测准确率直接影响着处理器整体的性能和功耗，当 BPU 预测准确率高，处理器流水线的空泡就比较少。但当 BPU 出现预测错误，不仅已执行的错误路径上的指令都被浪费，而且还需要冲刷流水线等来保证后续执行的正确性，这降低了处理器性能，并且带来额外的功耗。

BPU 的本质作用就是提炼程序执行过程中的分支行为特征，基于神经网络的动态预测便是一条技术路线。如今，基于感知机的分支预测器已应用在商业处理器中，AMD、三星的多款处理器中都包含了基于感知机的神经分支预测器。

还有很多研究提出将AI设计嵌入到芯片微架构的设计，但总体而言这类工作面临的一大难点是如何在硬件上可实现。很多设计方案由于硬件实现开销较大，实现复杂的AI算法存在困难而无法落地。

RISC-V 开源和可扩展的特性，使其十分合适成为 AI 技术嵌入芯片的实验架构。开源意味着可以集中多方力量进行技术实现的探索；可扩展的精简指令集意味着 RISC-V 架构先天具备更加模块化的设计理念，大大降低了嵌入 AI 的设计复杂度。

### 4.2 AI 辅助设计芯片

AI 设计芯片仍然处于起步阶段，各界都在积极探索，虽然已有一些亮点成果，但尚无被业界广泛使用的成熟解决方案。中科院计算所陈云霁团队曾在2022年《中国科学院院刊》上发表了一篇题为 “Chip Learning：从芯片设计到芯片学习”的观点文章，对 AI 设计芯片做了很好的总结与展望，文章认为 AI 设计芯片可以分成 3 个重要问题：

- 功能确定。根据用户意图确定系统正确的功能，并生成系统的准确表达。这种准确表达可以是硬件代码，也可以是表达式，也可以是真值表。这个问题对应着传统芯片设计流程的逻辑设计。
- 逻辑图生成。在准确表达的基础上生成电路的逻辑图表达，并在这张逻辑图上进行优化，最后生成物理无关的逻辑图表达。这个问题对应着传统芯片设计流程的电路设计。
- 物理图生成。在电路逻辑图基础上生成电路的具体物理版图，这等价于一种多种约束下（如面积、功耗、物理等限制）的图映射和优化问题。这个问题对应着传统芯片设计流程的物理设计。

针对上述问题，该文章由梳理了一系列从逻辑设计、电路设计、物理设计、验证测试各环节上的技术挑战。例如，验证测试环节要解决的核心挑战就是黑盒解决方案的精度保证，一方面端到端全自动设计的芯片是一个黑盒，验出 bug 后的可调试性将会是一个挑战，另一方面现有的  AI  技术也像个黑盒，缺乏可解释性，对于输出结果的精度无法保证与解释。

### 4.3 AI 辅助调优芯片

处理器芯片的最终性能取决于三个阶段的设计空间探索：

- 第一阶段是前端微架构设计，即探索各种微架构参数的最优组合提高处理器芯片性能。
- 第二阶段为后端物理设计，即探索不同的布局布线方案，不同的工艺参数提升 PPA。
- 第三阶段是芯片运行过程中根据软件特征动态调整芯片参数，或者反过来对软件进行优化从而让芯片运行更高效，这是一种软硬件协同优化思路，例如根据软件负载轻重来动态调整电压频率以降低处理器芯片运行功耗，也可以优化软件提高处理器运行效率以缩短软件运行时间。

AI 技术在上述三个阶段都能发挥重要作用，事实上近年来已有大量的相关工作。早在 2010 年左右，中科院计算所陈云霁研究员团队与南京大学周志华教授合作研究如何在微处理器设计阶段使用AI技术来提高设计空间探索的效率和效果。传统的方法依赖于大规模的周期精确架构模拟，非常耗时。研究团队提出了一种新的方法，能够利用未标记的设计配置来提高模型的预测准确性，是AI调优芯片方向的早期工作之一。2021 年，杜克大学陈怡然教授团队与ARM公司合作，使用一套统一的机器学习模型同时对设计和运行阶段的CPU功耗进行快速实时计算。

### 4.4 芯片支撑 AI

体系结构领域的专家们较早就认识到深度神经网络的潜在影响，从2010年便开始探索加速深度神经网络的处理器架构设计。

在 2012 年的 ISCA 上，Temam 教授提出了第一个机器学习加速器设计。随后，中科院计算所陈云霁研究员和 Temam 教授启动了一个学术合作项目：DianNao 家族加速器设计。DianNao 在 ISCA-2012 加速器的基础上增加了局部存储，解决了严重影响加速器性能的内存带宽限制。DianNao 家族加速器向全球展示了为 AI 应用设计专用加速器这条技术路线充满前景，此后各界都开始积极投入各类AI处理器芯片的设计。

2001年，斯坦福大学 Bill Dally 教授团队在处理器微结构旗舰期刊《IEEE Micro》上发表了一篇题为“Imagine: Media Processing with Streams”的论文正式介绍Imagine 项目，一个可加速多媒体应用的流处理器（Stream Processor）结构。这项工作很快引起了英伟达的关注，并向 Dally 教授伸出橄榄枝。随后在2003年，Dally教授担任英伟达的顾问，参与 GeForce 8800 系列 GPU 的微架构设计，指导如何在 GPU 中加入流处理器。2006 年 11 月，第一款 GeForce 8800 GTX GPU 发布，包含128个流处理器，单精度浮点运算性能达到 345.6GFLOPS，访存带宽 86.4GB/s，远高于同期 CPU 性能。2007 年，CUDA 1.0 正式发布，至此，英伟达的GPU 生态大厦的地基已基本构成，英伟达的 GPU 开始被称为 GPGPU（General Purpose GPU）。

### 4.5 AI 时代的 RISC-V 机遇

英伟达的 GPU 芯片和CUDA  软件生态已在当前AI浪潮中占据算力主导地位并形成高度垄断。打破这种垄断格局成为全球的共识，Google、Meta等企业都自研各自的AI处理器芯片。

彼时彼刻，x86_64 架构的垄断高高筑起了芯片领域的围墙，而横空出世的 ARM 架构仅需授权费就可以使用，用更低的墙打破了 x86_64 架构的垄断。如今 ARM 也发展成了芯片领域的大头。此时此刻，完全开源的 RISC-V 指令集架构则是彻底放弃了筑墙，掀起了自由指令集的浪潮。截至 2021 年底，全球 RISC-V 指令集的芯片累积卖出了100亿片。而 ARM 从 0 到 100 亿用了 15 年，RISC-V 只用了 7 年。

RISC-V 在 CPU 领域正迅速发展，而它同样在 GPU 领域具有突出的优势：

- 单一软件栈。传统AI处理器架构中CPU一般采用ARM、GPU往往采用Imagination，NPU自研，因而需要三套软件栈，如果都是采用基于RISC-V为基座来扩展AI指令集，那么就可以使用一套编译器和运行时。
- 无需考虑 DMA 操作的编程范式。当前AI处理器解决方案中需要通过 DMA 拷贝数据，基于RISC-V可实现紧耦合设计共享地址空间，从而避免显式的 DMA 操作。
- 更低延迟，更低功耗，更高效的Cache共享等等。

事实上，RISC-V 国际基金会也在积极布局。2023年  10 月，RISC-V 国际基金会成立AI/ML SIG组，推动全球 RISC-V AI 指令架构、基础软件和核心应用技术的研究与标准化。目前，RISC-V AI向量（Vector）已形成全球统一的标准，正在推动矩阵（Matrix）和张量（Tensor）标准制定。

若能形成统一的基于RISC-V的AI扩展指令集，那就有望在软件生态上形成合力，从而形成一个能平衡 CUDA 生态的新的AI软件生态。

## 参考

- [知乎 - 人工智能的发展可能会对芯片行业带来哪些变革影响？包云岗的回答](https://www.zhihu.com/question/827235247/answer/5133519791)
- [知乎 - 请问RISC-V架构适合用来开发AI处理器吗？史中的回答](https://www.zhihu.com/question/334303605/answer/3479852274)
- [知乎 - 困在“大型机”里的AI产业 ](https://zhuanlan.zhihu.com/p/11210303385)
- [龙蜥副理事长张东：加速推进 AI+OS 深度融合，打造最 AI 的服务器操作系统](https://mp.weixin.qq.com/s/i0XJU5QkwHlS0VF0C23HaA)
- [第五届中国云计算基础设施开发者大会 智能云基础设施分论坛：ZStack，云平台到 AI 原生平台的升级之路](https://blog.csdn.net/ZStack_io/article/details/143060036)
- [无需GPU！你的设备即将变身超级智能！](https://mp.weixin.qq.com/s/kQQ1I43WPkLWbrT9hMrl8Q)
